
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mvlearn.svg)](https://img.shields.io/pypi/pyversions/mvlearn.svg)
[![CircleCI](https://circleci.com/gh/mvlearn/mvlearn/tree/main.svg?style=shield)](https://app.circleci.com/pipelines/github/mvlearn/mvlearn)
[![codecov](https://codecov.io/gh/mvlearn/mvlearn/branch/main/graph/badge.svg)](https://codecov.io/gh/mvlearn/mvlearn)
[![PyPI version](https://badge.fury.io/py/mvlearn.svg)](https://badge.fury.io/py/mvlearn)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/mvlearn.svg)](https://anaconda.org/conda-forge/mvlearn)
[![License](https://img.shields.io/github/license/mvlearn/mvlearn)](https://opensource.org/licenses/MIT)
[![arXiv shield](https://img.shields.io/badge/arXiv-2005.11890-red.svg?style=flat)](https://arxiv.org/abs/2005.11890)

<p align="center">
  <img width=300 src="docs/figures/mvlearn-logo-transparent-grey.png" />
</p>

`mvlearn` is an open-source Python software package for multiview learning tools.

- [**Installation Guide**](https://mvlearn.github.io/install.html)
- [**Documentation**](https://mvlearn.github.io/)
- [**Examples**](https://mvlearn.github.io/auto_examples/index.html)
- [**Source Code**](https://github.com/mvlearn/mvlearn/tree/main/mvlearn)
- [**Issues**](https://github.com/mvlearn/mvlearn/issues)
- [**Contribution Guide**](https://mvlearn.github.io/contributing.html)
- [**Changelog**](https://mvlearn.github.io/changelog.html)

`mvlearn` aims to serve as a community-driven open-source software package that offers reference implementations for algorithms and methods related to multiview learning (machine learning in settings where there are multiple incommensurate views or feature sets for each sample). It brings together the most widely-used tools in this setting with a standardized scikit-learn like API, well tested code and high-quality documentation. Doing so, we aim to facilitate application, extension, and comparison of methods, and offer a foundation for research into new multiview algorithms. We welcome new contributors and the addition of methods with proven efficacy and current use.
